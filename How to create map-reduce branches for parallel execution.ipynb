{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create map-reduce branches for parallel execution\n",
    "\n",
    "A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be?\n",
    "\n",
    "LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph.\n",
    "\n",
    "Let's see what this looks like in action! We'll put together a toy example of generating a list of words, and then writing a joke about each word, and then judging what the best joke is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in ./env/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2 in ./env/lib/python3.12/site-packages (from langgraph) (0.2.10)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./env/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./env/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in ./env/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.82)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./env/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./env/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (2.7.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./env/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./env/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.2->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.2->langgraph) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.2->langgraph) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2024.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_topics': {'subjects': ['cat', 'dog', 'lion', 'elephant', 'monkey']}}\n",
      "{'generate_joke': {'jokes': [\"Why don't lions like to play cards in the jungle? Too many cheetahs!\"]}}\n",
      "{'generate_joke': {'jokes': [\"Why did the dog sit in the shade? Because he didn't want to be a hot dog!\"]}}\n",
      "{'generate_joke': {'jokes': ['Why did the elephant bring a suitcase to the picnic? Because he wanted to pack his trunk!']}}\n",
      "{'generate_joke': {'jokes': ['Why was the cat sitting on the computer? Because it wanted to keep an eye on the mouse!']}}\n",
      "{'generate_joke': {'jokes': ['Why did the monkey like the banana? Because it had appeal!']}}\n",
      "{'best_joke': {'best_selected_joke': \"Why did the dog sit in the shade? Because he didn't want to be a hot dog!\"}}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Model and prompts\n",
    "# Define model and prompts we will use\n",
    "subjects_prompt = \"\"\"Generate a comma separated list of between 2 and 5 {topic}.\"\"\"\n",
    "joke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\n",
    "best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one.\n",
    "\n",
    "{jokes}\"\"\"\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Graph components: define the components that will make up the graph\n",
    "\n",
    "\n",
    "# This will be the overall state of the main graph.\n",
    "# It will contain a topic (which we expect the user to provide)\n",
    "# and then will generate a list of subjects, and then a joke for\n",
    "# each subject\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the jokes we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    best_selected_joke: str\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# subjects to in order to generate a joke\n",
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "# This is the function we will use to generate the subjects of the jokes\n",
    "def generate_topics(state: OverallState):\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    return {\"subjects\": response.subjects}\n",
    "\n",
    "# Here we generate a joke, given a subject\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    return {\"jokes\": [response.joke]}\n",
    "\n",
    "# Here we define the logic to map out over the generated subjects\n",
    "# We will use this an edge in the graph\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "\n",
    "# Here we will judge the best joke\n",
    "def best_joke(state: OverallState):\n",
    "    jokes = \"\\n\\n\".format()\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}\n",
    "\n",
    "# Construct the graph: here we put everything together to construct our graph\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_topics\", generate_topics)\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "graph.set_entry_point(\"generate_topics\")\n",
    "\n",
    "graph.add_conditional_edges(\"generate_topics\", continue_to_jokes)\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "app = graph.compile()\n",
    "\n",
    "# Call the graph: here we call it to generate a list of jokes\n",
    "for s in app.stream({\"topic\": \"animals\"}):\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
