{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to view and update past graph stateÂ¶\n",
    "\n",
    "Once you start checkpointing your graphs, you can easily get or update the state of the agent at any point in time. This permits a few things:\n",
    "- You can surface a state during an interrupt to a user to let them accept an action.\n",
    "- You can rewind the graph to reproduce or avoid issues.\n",
    "- You can modify the state to embed your agent into a larger system, or to let the user better control its actions.\n",
    "\n",
    "The key methods used for this functionality are:\n",
    "- get_state: fetch the values from the target config\n",
    "- update_state: apply the given values to the target state\n",
    "\n",
    "Note: this requires passing in a checkpointer.\n",
    "\n",
    "Below is a quick example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -U langgraph langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# Set up the tool\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    # Don't let the LLM know this though ðŸ˜Š\n",
    "    return [\n",
    "        f\"I looked up: {query}. Result: It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# Set up the model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "model = model.bind_tools(tools)\n",
    "\n",
    "# Define nodes and conditional edges\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool_call[\"name\"],\n",
    "        tool_input=tool_call[\"args\"],\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a ToolMessage\n",
    "    tool_message = ToolMessage(\n",
    "        content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [tool_message]}\n",
    "\n",
    "# Build the graph\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Set up memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with the Agent\n",
    "We can now interact with the agent. Let's ask it for the weather in SF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Use the search tool to look up the weather in SF\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search (call_QaaKXnOsCZEB3iSPobymlgpu)\n",
      " Call ID: call_QaaKXnOsCZEB3iSPobymlgpu\n",
      "  Args:\n",
      "    query: weather in San Francisco\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search\n",
      "\n",
      "[\"I looked up: weather in San Francisco. Result: It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco is currently sunny. However, if you're a Gemini, you might want to be cautious!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"Use the search tool to look up the weather in SF\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking history\n",
    "Let's browse the history of this thread, from start to finish.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'messages': []}, next=('__start__',), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef33da8-328e-64fc-bfff-cf9a0af78b08'}}, metadata={'source': 'input', 'step': -1, 'writes': {'messages': [HumanMessage(content='Use the search tool to look up the weather in SF')]}}, created_at='2024-06-26T16:38:26.815704+00:00', parent_config=None)\n",
      "--\n",
      "StateSnapshot(values={'messages': [HumanMessage(content='Use the search tool to look up the weather in SF', id='d799a0e7-9a1b-4997-9f63-934cb6f72d78')]}, next=('agent',), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef33da8-3294-61c2-8000-1e1504065b4e'}}, metadata={'source': 'loop', 'step': 0, 'writes': None}, created_at='2024-06-26T16:38:26.818082+00:00', parent_config=None)\n",
      "--\n",
      "StateSnapshot(values={'messages': [HumanMessage(content='Use the search tool to look up the weather in SF', id='d799a0e7-9a1b-4997-9f63-934cb6f72d78'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QaaKXnOsCZEB3iSPobymlgpu', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 54, 'total_tokens': 70}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0', tool_calls=[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_QaaKXnOsCZEB3iSPobymlgpu'}], usage_metadata={'input_tokens': 54, 'output_tokens': 16, 'total_tokens': 70})]}, next=('action',), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef33da8-3984-678e-8001-de497fadedc1'}}, metadata={'source': 'loop', 'step': 1, 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QaaKXnOsCZEB3iSPobymlgpu', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 54, 'total_tokens': 70}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0', tool_calls=[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_QaaKXnOsCZEB3iSPobymlgpu'}], usage_metadata={'input_tokens': 54, 'output_tokens': 16, 'total_tokens': 70})]}}}, created_at='2024-06-26T16:38:27.545685+00:00', parent_config=None)\n",
      "--\n",
      "StateSnapshot(values={'messages': [HumanMessage(content='Use the search tool to look up the weather in SF', id='d799a0e7-9a1b-4997-9f63-934cb6f72d78'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QaaKXnOsCZEB3iSPobymlgpu', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 54, 'total_tokens': 70}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0', tool_calls=[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_QaaKXnOsCZEB3iSPobymlgpu'}], usage_metadata={'input_tokens': 54, 'output_tokens': 16, 'total_tokens': 70}), ToolMessage(content='[\"I looked up: weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini ðŸ˜ˆ.\"]', name='search', id='fa8ee48a-e8fa-4a53-949e-e7c71bf2bf89', tool_call_id='call_QaaKXnOsCZEB3iSPobymlgpu')]}, next=('agent',), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef33da8-3989-6dec-8002-2126a81c829e'}}, metadata={'source': 'loop', 'step': 2, 'writes': {'action': {'messages': [ToolMessage(content='[\"I looked up: weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini ðŸ˜ˆ.\"]', name='search', id='fa8ee48a-e8fa-4a53-949e-e7c71bf2bf89', tool_call_id='call_QaaKXnOsCZEB3iSPobymlgpu')]}}}, created_at='2024-06-26T16:38:27.547894+00:00', parent_config=None)\n",
      "--\n",
      "StateSnapshot(values={'messages': [HumanMessage(content='Use the search tool to look up the weather in SF', id='d799a0e7-9a1b-4997-9f63-934cb6f72d78'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QaaKXnOsCZEB3iSPobymlgpu', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 54, 'total_tokens': 70}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0', tool_calls=[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_QaaKXnOsCZEB3iSPobymlgpu'}], usage_metadata={'input_tokens': 54, 'output_tokens': 16, 'total_tokens': 70}), ToolMessage(content='[\"I looked up: weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini ðŸ˜ˆ.\"]', name='search', id='fa8ee48a-e8fa-4a53-949e-e7c71bf2bf89', tool_call_id='call_QaaKXnOsCZEB3iSPobymlgpu'), AIMessage(content=\"The weather in San Francisco is currently sunny. However, if you're a Gemini, you might want to be cautious!\", response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 109, 'total_tokens': 134}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d91366ab-ca02-4e86-ac09-eab6d9254745-0', usage_metadata={'input_tokens': 109, 'output_tokens': 25, 'total_tokens': 134})]}, next=(), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef33da8-4121-633e-8003-4f49f738a57f'}}, metadata={'source': 'loop', 'step': 3, 'writes': {'agent': {'messages': [AIMessage(content=\"The weather in San Francisco is currently sunny. However, if you're a Gemini, you might want to be cautious!\", response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 109, 'total_tokens': 134}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d91366ab-ca02-4e86-ac09-eab6d9254745-0', usage_metadata={'input_tokens': 109, 'output_tokens': 25, 'total_tokens': 134})]}}}, created_at='2024-06-26T16:38:28.343882+00:00', parent_config=None)\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "all_states = []\n",
    "for state in app.get_state_history(config):\n",
    "    print(state)\n",
    "    all_states.append(state)\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay a state\n",
    "We can go back to any of these states and restart the agent from there! Let's go back to right before the tool call gets executed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='Use the search tool to look up the weather in SF', id='d799a0e7-9a1b-4997-9f63-934cb6f72d78'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QaaKXnOsCZEB3iSPobymlgpu', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 54, 'total_tokens': 70}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0', tool_calls=[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_QaaKXnOsCZEB3iSPobymlgpu'}], usage_metadata={'input_tokens': 54, 'output_tokens': 16, 'total_tokens': 70})]}, next=('action',), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef33da8-3984-678e-8001-de497fadedc1'}}, metadata={'source': 'loop', 'step': 1, 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QaaKXnOsCZEB3iSPobymlgpu', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 54, 'total_tokens': 70}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0', tool_calls=[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_QaaKXnOsCZEB3iSPobymlgpu'}], usage_metadata={'input_tokens': 54, 'output_tokens': 16, 'total_tokens': 70})]}}}, created_at='2024-06-26T16:38:27.545685+00:00', parent_config=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_replay = all_states[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Use the search tool to look up the weather in SF', id='d799a0e7-9a1b-4997-9f63-934cb6f72d78'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QaaKXnOsCZEB3iSPobymlgpu', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 54, 'total_tokens': 70}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0', tool_calls=[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_QaaKXnOsCZEB3iSPobymlgpu'}], usage_metadata={'input_tokens': 54, 'output_tokens': 16, 'total_tokens': 70})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_replay.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('action',)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_replay.next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replay from this place we just need to pass its config back to the agent. Notice that it just resumes from right where it left all - making a tool call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [ToolMessage(content='[\"I looked up: weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini ðŸ˜ˆ.\"]', name='search', tool_call_id='call_QaaKXnOsCZEB3iSPobymlgpu')]}\n",
      "{'messages': [AIMessage(content=\"The weather in San Francisco is sunny, but there's a playful warning for Geminis!\", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 109, 'total_tokens': 129}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-75040f3e-6e4d-4545-b9c6-c8726e0518b2-0', usage_metadata={'input_tokens': 109, 'output_tokens': 20, 'total_tokens': 129})]}\n"
     ]
    }
   ],
   "source": [
    "for event in app.stream(None, to_replay.config):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branch off a past state\n",
    "Using LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\n",
    "\n",
    "Let's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now get the last message in the state\n",
    "# This is the one with the tool calls that we want to update\n",
    "last_message = to_replay.values['messages'][-1]\n",
    "\n",
    "# Let's now update the args for that tool call\n",
    "last_message.tool_calls[0]['args'] = {'query': 'current weather in SF'}\n",
    "\n",
    "branch_config = app.update_state(\n",
    "    to_replay.config, {\"messages\": [last_message]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then invoke with this new branch_config to resume running from here with changed state. We can see from the log that the tool was called with different input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [ToolMessage(content='[\"I looked up: current weather in SF. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini ðŸ˜ˆ.\"]', name='search', tool_call_id='call_QaaKXnOsCZEB3iSPobymlgpu')]}\n",
      "{'messages': [AIMessage(content=\"The current weather in San Francisco is sunny. But it seems like there's a playful message for Geminis to look out for!\", response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 109, 'total_tokens': 137}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a4e30258-66e6-454a-a530-ab47c24fb667-0', usage_metadata={'input_tokens': 109, 'output_tokens': 28, 'total_tokens': 137})]}\n"
     ]
    }
   ],
   "source": [
    "for event in app.stream(None, branch_config):\n",
    "    for v in event.values():\n",
    "        print(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could update the state to not even call a tool!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Let's now get the last message in the state\n",
    "# This is the one with the tool calls that we want to update\n",
    "last_message = to_replay.values['messages'][-1]\n",
    "\n",
    "# Let's now get the ID for the last message, and create a new message with that ID.\n",
    "new_message = AIMessage(content=\"its warm!\", id=last_message.id)\n",
    "\n",
    "branch_config = app.update_state(\n",
    "    to_replay.config, {\"messages\": [new_message]},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_state = app.get_state(branch_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Use the search tool to look up the weather in SF', id='d799a0e7-9a1b-4997-9f63-934cb6f72d78'),\n",
       "  AIMessage(content='its warm!', id='run-0c1891d4-fd24-408f-882e-6bcdebea049c-0')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch_state.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch_state.next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the snapshot was updated and now correctly reflects that there is no next step.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
